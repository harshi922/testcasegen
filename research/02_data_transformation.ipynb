{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\testcasegen\\\\research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\testcasegen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harsh\\anaconda3\\envs\\testcasegen\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    data_path: Path\n",
    "    embedding_model: str\n",
    "    model_url: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmRAGtestcasegen.constants import *\n",
    "from llmRAGtestcasegen.utils.common import read_yaml, create_directories, download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import pinecone\n",
    "from llmRAGtestcasegen.logging import logger\n",
    "import time\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")\n",
    "import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigManager:\n",
    "    def __init__(\n",
    "            self, \n",
    "            config__filepath = CONFIG_FILE_PATH,\n",
    "            params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config__filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root]) \n",
    "    def download_file(self, url):\n",
    "        current_dir = os.getcwd()\n",
    "        if not os.path.exists(os.path.join(current_dir + \"unixcoder.py\")):\n",
    "            try:\n",
    "                filename = wget.download(url)\n",
    "                print(f\"\\nDownloaded: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "        else:\n",
    "            print(\"File already exists\")\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        dataconfig =  DataTransformationConfig(\n",
    "            data_path= config.data_path,\n",
    "            embedding_model = config.embedding_model,\n",
    "            model_url = config.model_url\n",
    "        )\n",
    "        download_file(config.model_url)\n",
    "        return dataconfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_ai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unixcoder import UniXcoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.java_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            language=Language.JAVA, chunk_size=500, chunk_overlap=150\n",
    "        )\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.embedding_model = UniXcoder(\"microsoft/unixcoder-base\")\n",
    "        self.embedding_model.to(self.device)\n",
    "\n",
    "    def get_embeddings_codebert(self, code_corpus):\n",
    "        embeddings = []\n",
    "        for code in code_corpus:\n",
    "            tokens_ids = self.embedding_model.tokenize([code], mode=\"<encoder-only>\")\n",
    "            source_ids = torch.tensor(tokens_ids).to(self.device)\n",
    "            _, nl_embedding = self.embedding_model(source_ids)\n",
    "            norm_nl_embedding = torch.nn.functional.normalize(nl_embedding, p=2, dim=1)\n",
    "            norm_nl_embedding = norm_nl_embedding.detach().cpu().numpy()[0].tolist()\n",
    "            embeddings.append(norm_nl_embedding)\n",
    "        return embeddings\n",
    "\n",
    "    def create_embeddings(self):\n",
    "        pinecone.init(api_key=pinecone_api_key, environment='gcp-starter')\n",
    "        index_name = 'd4jcodebertembeds'\n",
    "\n",
    "        if index_name not in pinecone.list_indexes():\n",
    "            pinecone.create_index(index_name, dimension=768, metric='cosine')\n",
    "            # Wait for index to initialize\n",
    "            while not pinecone.describe_index(index_name).status['ready']:\n",
    "                time.sleep(1)\n",
    "        index = pinecone.Index(index_name)\n",
    "        logger.info(f\"Index {index_name} created successfully with info: {index.describe_index_stats()}\")\n",
    "\n",
    "        # Load and preprocess data\n",
    "        df = pd.read_csv(Path(self.config.data_path), usecols=['Class', 'Methods', 'Repository', 'TestCase'])\n",
    "        df = df[df['Repository'].isin(['commons-jxpath', 'commons-csv'])].reset_index(drop=True)\n",
    "        \n",
    "        # Split each code file into chunks\n",
    "        new_rows = []\n",
    "        for _, x in df.iterrows():\n",
    "            java_chunks = self.java_splitter.create_documents([x['Methods']])\n",
    "            new_rows.extend({'Class': x['Class'], 'Methods': chunk.page_content, 'Repository': x['Repository']}\n",
    "                            for chunk in java_chunks)\n",
    "        \n",
    "        new_df = pd.DataFrame(new_rows)\n",
    "        logger.info(f\"Dataframe shape: {new_df.shape}\")\n",
    "\n",
    "        # Process and upload in batches\n",
    "        batch_size = 4\n",
    "        for i in tqdm(range(0, len(new_df), batch_size)):\n",
    "            batch = new_df.iloc[i:i + batch_size]\n",
    "            ids = [str(j) for j in batch.index]\n",
    "            texts = batch['Methods'].tolist()\n",
    "            embeddings = self.get_embeddings_codebert(texts)\n",
    "\n",
    "            metadata = [{'Class': row['Class'], 'Methods': row['Methods'], 'Repository': row['Repository']}\n",
    "                        for _, row in batch.iterrows()]\n",
    "            try:\n",
    "                index.upsert(vectors=list(zip(ids, embeddings, metadata)))\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error upserting batch {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        logger.info(f\"Uploading completed: {index.describe_index_stats()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-31 11:40:46,944: INFO: common: Reading yaml file from config\\config.yaml loaded suucessfully]\n",
      "[2024-10-31 11:40:46,945: INFO: common: Reading yaml file from params.yaml loaded suucessfully]\n",
      "[2024-10-31 11:40:46,946: INFO: common: Directory created at artifacts]\n",
      "\n",
      "Downloaded: unixcoder.py\n",
      "[2024-10-31 11:40:47,304: INFO: common: File downloaded from https://raw.githubusercontent.com/microsoft/CodeBERT/master/UniXcoder/unixcoder.py and saved]\n",
      "[2024-10-31 11:40:51,449: INFO: 3570973218: Index d4jcodebertembeds created successfully with info: {'dimension': 768,\n",
      " 'index_fullness': 0.00258,\n",
      " 'namespaces': {'': {'vector_count': 258}},\n",
      " 'total_vector_count': 258}]\n",
      "[2024-10-31 11:40:51,643: INFO: 3570973218: Dataframe shape: (258, 3)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65/65 [00:41<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-31 11:41:33,125: INFO: 3570973218: Uploading completed: {'dimension': 768,\n",
      " 'index_fullness': 0.00258,\n",
      " 'namespaces': {'': {'vector_count': 258}},\n",
      " 'total_vector_count': 258}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    config = ConfigManager()\n",
    "    data_trans_config = config.get_data_transformation_config()\n",
    "    data_transformer = DataTransformation(data_trans_config)\n",
    "    data_transformer.create_embeddings()\n",
    "except Exception as e:\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testcasegen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
